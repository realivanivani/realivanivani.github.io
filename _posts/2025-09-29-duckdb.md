---
title: 'DuckDB - Analytics for not-so-big data with DuckDB'
date: 2025-09-29
permalink: /posts/2025/09/blog-post-1/
tags:
  - DuckDB
  - SQL
  - OLAP
  - engine
  - python
---
In analytics engineering, tooling choices are often framed as binary: either a transactional database like Postgres, or a distributed engine like Spark. In reality, a large portion of analytical workloads sit in between. The data fits on a laptop or a single VM, but the queries are heavy: wide tables, joins across fact datasets, window functions, and time-based aggregations.

This is where **DuckDB** fits: not as a toy database, and not as a replacement for a data warehouse, but as a **serious single-node analytical engine**.

DuckDB is often described as *“SQLite for analytics”*. That description is accurate, but incomplete. What matters more is *why* this design works so well for modern ETL and analytics workflows.

<img width="1024" height="1024" alt="image" src="https://github.com/user-attachments/assets/ede453a5-e04e-4454-9e38-715e73af9ac7" />

---

## What DuckDB Is (and Why It’s Built This Way)

DuckDB is an open-source, embeddable **OLAP SQL database**, written in C++ and first released in 2019. Like SQLite, it runs as a single binary inside your application process — no server, no daemon, no external dependencies. This design choice is not about convenience alone; it fundamentally shapes how DuckDB is used.

From an analytics engineering perspective, this means:

* You can treat DuckDB as a **library**, not infrastructure.
* Analytical logic can live closer to code, version control, and tests.
* Local development mirrors production logic more closely than ad-hoc Pandas scripts.

The key architectural distinction is storage layout.

---

## Columnar Storage: Why DuckDB Behaves Like an Analytics Engine

Traditional relational databases (Postgres, MySQL, SQLite) store data **row by row**. This is ideal for transactional workloads where full records are read or updated frequently (**OLTP**).

Analytical workloads behave differently. They scan large datasets, often touching only a few columns at a time, and apply aggregations across many rows.

DuckDB uses **columnar storage**, meaning:

* Only referenced columns are read from disk
* Aggregations operate on contiguous memory blocks
* Cache efficiency is significantly higher

This is why DuckDB performs well on time-series data, event logs, IoT data, and fact tables — the same classes of data typically modeled in analytics layers.

---

## Vectorized Execution and Parallelism (Why It’s Fast on One Machine)

DuckDB’s execution engine is **vectorized**. Instead of processing data row by row, it operates on batches (vectors) of values at a time. This allows the engine to:

* Reduce interpreter overhead
* Apply CPU-level optimizations
* Parallelize work across cores by default

This is a key reason DuckDB can process hundreds of millions or even billions of rows on a single machine, as demonstrated in the NDC Oslo talk *Analytics for not-so-big data with DuckDB*
([https://www.youtube.com/watch?v=3iy9qaGopyw](https://www.youtube.com/watch?v=3iy9qaGopyw)).

For analytics engineers, this matters because it enables **local validation of transformations** that would otherwise require deploying to a warehouse just to test performance characteristics.

---

## SQL as the Primary Interface (Not an Afterthought)

DuckDB speaks standard SQL. There is no custom DSL, no hidden API layer. Creating tables, inserting data, querying results — all behave as expected:

```sql
CREATE TABLE users (
    id INTEGER,
    name VARCHAR
);

INSERT INTO users VALUES (1, 'Jeff');

SELECT name FROM users;
```

This makes DuckDB particularly attractive for SQL-first teams, especially those already using dbt or warehouse-native transformations. Logic written in DuckDB SQL is highly portable.

You can start interacting immediately via the CLI:

```bash
duckdb
```

---

## Querying Files Directly: Rethinking “Extract” in ETL

One of DuckDB’s most impactful features for ETL workflows is its ability to **query files directly**, without an import step.

```sql
SELECT * FROM 'data.csv';
SELECT * FROM 'data.parquet';
```

This shifts how you think about extraction:

* CSV and Parquet become **first-class query sources**
* You can validate schema, distributions, and data quality before loading
* Intermediate staging tables are often unnecessary

Because DuckDB understands Parquet metadata and uses columnar execution, it will only read the columns required to answer a query — even when the file lives remotely (HTTP or object storage).

This is a practical alternative to spinning up temporary databases just to inspect raw drops.

---

## DuckDB as a SQL Layer on Top of Pandas

DuckDB integrates tightly with Python and Pandas, which makes it useful as a **bridge between exploratory analysis and production SQL**.

```python
import duckdb
import pandas as pd

df = pd.DataFrame({
    "employee": ["Alice", "Bob", "Charlie"],
    "salary": [120000, 90000, 135000],
    "department": ["IT", "HR", "IT"]
})

duckdb.query("""
    SELECT department,
           AVG(salary) AS avg_salary,
           COUNT(*)    AS headcount
    FROM df
    GROUP BY department
""").to_df()
```

For more structured codebases, explicit registration is preferable:

```python
con = duckdb.connect()
con.register("employees", df)

con.execute("""
    SELECT *
    FROM employees
    WHERE salary > 100000
""").fetchdf()
```

This pattern works well when:

* Prototyping transformations in notebooks
* Gradually moving logic into SQL
* Avoiding complex Pandas pipelines that are hard to test and review

(For a practical walkthrough, see ArjanCodes: *Stop Struggling with DataFrames – Try DuckDB*.)

---

## DuckDB as a Lightweight ETL Engine

DuckDB is often faster and simpler than custom Python scripts for format conversion and transformation tasks.

Example: converting a large CSV into Parquet.

```bash
duckdb -c "
COPY (
    SELECT *
    FROM read_csv_auto('events_raw.csv')
)
TO 'events.parquet'
(FORMAT PARQUET);
"
```

Why this works well in practice:

* Automatic schema inference
* Parallel execution across CPU cores
* Deterministic, SQL-defined transformations
* Minimal memory overhead compared to Pandas

For analytics engineers, this fits naturally into **ELT-style pipelines**, where raw data is preserved and refined through SQL transformations.

---

## Analytical Aggregations and Time-Based Queries

DuckDB excels at analytical queries on time-series and event data:

```sql
SELECT
    symbol,
    AVG(price),
    MAX(price),
    MIN(price)
FROM 'stocks.parquet'
WHERE date > now() - interval '1 day'
GROUP BY symbol;
```

Under the hood, this query is executed using vectorized batches and multi-threaded execution, which explains why it scales well even without distributed infrastructure.

---

## Built-In Introspection for Data and Queries

DuckDB includes several features that are especially useful during modeling and optimization:

* `DESCRIBE table` – inspect schema and types
* `SUMMARIZE table` – get column-level statistics
* `EXPLAIN ANALYZE` – see the physical query plan and execution time

These tools reduce guesswork when tuning transformations or validating assumptions about data distributions.

---

## Adoption and Real-World Usage

DuckDB is not an experimental project. It is used by organizations such as **Meta, Google, and Airbnb**, typically embedded inside data tools, notebooks, or internal pipelines rather than exposed as a user-facing database.

This usage pattern reinforces its core strength: **analytical computation without operational overhead**.

(Overview reference: Fireship – *DuckDB in 100 Seconds*: [https://www.youtube.com/watch?v=uHm6FEb2Re4](https://www.youtube.com/watch?v=uHm6FEb2Re4))

---

## Where DuckDB Fits in an Analytics Stack

DuckDB does not replace a data warehouse. It complements one.

It is particularly well-suited for:

* Local development of analytical SQL
* Data profiling and validation
* Lightweight ETL and file transformations
* Reproducible analytics without infrastructure setup

If your current workflow oscillates between fragile Pandas scripts and heavyweight distributed systems, DuckDB often fills the gap more cleanly than either extreme.

---

### Further References

* **Analytics for not-so-big data with DuckDB (NDC Oslo)**
  [https://www.youtube.com/watch?v=3iy9qaGopyw](https://www.youtube.com/watch?v=3iy9qaGopyw)
* **DuckDB in 100 Seconds (Fireship)**
  [https://www.youtube.com/watch?v=uHm6FEb2Re4](https://www.youtube.com/watch?v=uHm6FEb2Re4)
* **Stop Struggling with DataFrames – Try DuckDB (ArjanCodes)**
  [https://www.youtube.com/watch?v=8SYQtpSk_OI](https://www.youtube.com/watch?v=8SYQtpSk_OI)

---

If you want next steps, I can:

* Add a **DuckDB vs Postgres vs Spark** comparison from an ETL perspective
* Extend this into a **dbt-style modeling example using DuckDB**
* Turn it into a **hands-on tutorial repo structure**

Just tell me the intended audience and format.



## DuckDB: Analytics for “Not-So-Big” Data That Still Deserves Proper Engineering

In data teams, tooling choices often jump straight from Postgres to Spark. Once datasets stop fitting comfortably into a transactional database, the reflex is to introduce distributed systems, clusters, schedulers, and a lot of operational overhead. In practice, many analytical workloads don’t need any of that.

If your data fits on a laptop or a single VM, but your queries involve wide tables, heavy joins, window functions, and aggregations over tens or hundreds of millions of rows, you’re firmly in *analytical* territory — just not *big data* territory.

This is exactly where **DuckDB** fits.

DuckDB is best understood as a **local, embedded OLAP engine** that behaves like a serious analytical database, not a toy replacement for SQLite. For analytics engineers and SQL-first developers, it can simplify ETL pipelines, reduce dependency sprawl, and dramatically speed up local iteration.

---

## What DuckDB Actually Is (and Isn’t)

DuckDB is an **in-process, column-oriented relational database** optimized for analytical queries. You link it to your application or script, and it runs in the same process — no daemon, no service, no cluster.

A few implications that matter in practice:

* It is **not** designed for high-concurrency transactional workloads.
* It **is** designed for scanning large datasets, doing joins, aggregations, and complex analytical SQL efficiently.
* It assumes data locality (local disk, object storage, HTTP) rather than a constantly mutating OLTP store.

Key characteristics that are relevant for analytics engineering:

* **Columnar execution** – only the columns referenced in the query are read.
* **Vectorized processing** – operations are applied in batches, not row by row.
* **Single-file database** – easy to version, move, or recreate.
* **First-class file access** – CSV, Parquet, JSON, Excel, S3, HTTP.

If you’ve ever used Postgres for analytics and felt that “this should be faster,” DuckDB was built to answer that exact complaint.

---

## Using DuckDB as a SQL Layer on Top of DataFrames

One of the most practical entry points is using DuckDB as a **query engine for Pandas DataFrames**. This is especially useful when Pandas logic starts to become unreadable or slow.

```python
import duckdb
import pandas as pd

df = pd.DataFrame({
    "employee": ["Alice", "Bob", "Charlie"],
    "salary": [120000, 90000, 135000],
    "department": ["IT", "HR", "IT"]
})

duckdb.query("""
    SELECT department,
           AVG(salary) AS avg_salary,
           COUNT(*)    AS headcount
    FROM df
    GROUP BY department
""").to_df()
```

For exploratory analysis or notebook-based work, this immediately replaces chains of `groupby`, `agg`, and `merge` calls with readable SQL.

In production-grade scripts, explicit registration is cleaner and more predictable:

```python
con = duckdb.connect()
con.register("employees", df)

result = con.execute("""
    SELECT *
    FROM employees
    WHERE salary > 100000
""").fetchdf()
```

This pattern is powerful because it lets you **prototype transformations in Pandas** and **lock them down in SQL** once they stabilize — a workflow that maps well to dbt-style thinking.

---

## DuckDB for Serious Analytical Workloads

DuckDB’s performance story becomes more interesting once you stop thinking in “DataFrame size” and start thinking in **scan volume**.

In the NDC Oslo talk *Analytics for not-so-big data with DuckDB*, large fact tables (on the order of **hundreds of millions to a billion rows**) are queried on a single machine with response times in seconds. The key is that DuckDB assumes analytical access patterns and optimizes for them aggressively.
(Reference: NDC Oslo talk by David Ostrovsky – [https://www.youtube.com/watch?v=3iy9qaGopyw](https://www.youtube.com/watch?v=3iy9qaGopyw))

For example, pivot-style reporting queries that are painful in traditional databases are straightforward:

```sql
PIVOT lineitem
ON returnflag
USING SUM(extendedprice);
```

For analytics engineers, this matters because it enables **local validation of logic** that would otherwise require deploying to a warehouse or cluster just to test query semantics.

---

## Querying Data Where It Lives (Without ETL First)

A common anti-pattern in analytics pipelines is “load everything first, then filter.” DuckDB encourages the opposite.

You can query Parquet files directly from disk, object storage, or even over HTTP:

```sql
SELECT
    COUNT(*) AS trips,
    AVG(distance) AS avg_distance
FROM 'https://raw.githubusercontent.com/.../trips.parquet'
WHERE pickup_year = 2023;
```

Because Parquet is columnar and DuckDB understands its metadata, only the required columns and row groups are read. This makes DuckDB a strong choice for:

* Profiling raw data drops
* Validating upstream exports
* Lightweight federation across files

In many cases, this replaces the need for a temporary staging database entirely.

---

## DuckDB as a Zero-Friction ETL Engine

For ETL tasks, DuckDB shines as a **fast, declarative transformer**. Instead of writing imperative scripts, you express transformations once in SQL and let the engine handle parallelism and memory management.

A simple but common example: converting large CSV files into Parquet.

```bash
duckdb -c "
COPY (
    SELECT *
    FROM read_csv_auto('events_raw.csv')
)
TO 'events.parquet'
(FORMAT PARQUET);
"
```

This approach has several advantages over Pandas-based ETL:

* Automatic type inference
* Parallel execution
* Lower memory pressure
* Reproducible transformations

For analytics engineers, this aligns well with **ELT-style pipelines**, where raw data is ingested as-is and refined through SQL transformations.

---

## Built-In Data Introspection

DuckDB includes practical inspection tools that reduce boilerplate:

* `DESCRIBE table` – schema and types
* `SUMMARIZE table` – distribution statistics for every column
* `EXPLAIN ANALYZE` – physical query plan with timings

These are especially useful when optimizing transformations or validating assumptions about data distributions before modeling.

---

## Where DuckDB Fits in an Analytics Stack

DuckDB is not a replacement for a data warehouse. It *is* a strong complement to one.

Typical use cases where it fits naturally:

* Local development and testing of analytical SQL
* Lightweight ETL and format conversion
* Ad-hoc analysis on large files
* Reproducible analytical scripts without infrastructure overhead

If you think of it as “SQLite, but for analytics,” you’ll underuse it. If you think of it as a **single-node analytical engine**, it becomes much more interesting.

---

### Further References

* **Analytics for not-so-big data with DuckDB (NDC Oslo)**
  [https://www.youtube.com/watch?v=3iy9qaGopyw](https://www.youtube.com/watch?v=3iy9qaGopyw)
* **Stop Struggling with DataFrames – Try DuckDB (ArjanCodes)**
  [https://www.youtube.com/watch?v=8SYQtpSk_OI](https://www.youtube.com/watch?v=8SYQtpSk_OI)

---

If you want, I can:

* Reframe this as a **blog for analytics engineers**
* Add a **DuckDB vs Postgres / Spark comparison table**
* Extend it with **dbt-style transformation examples**
* Add a **real ETL case study** (CSV → Parquet → analytical model)

Just tell me the target audience and publication platform.

