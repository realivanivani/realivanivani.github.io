---
title: 'DuckDB - Analytics for not-so-big data with DuckDB'
date: 2025-09-29
permalink: /posts/2025/09/blog-post-1/
tags:
  - DuckDB
  - SQL
  - query
  - engine
  - python
--- 

In the world of data engineering and analytics, the phrase "Big Data" often leads people to reach for heavy-duty tools like Apache Spark or Snowflake. But what if your data fits on a laptop, yet is too "medium-sized" for a traditional database like MySQL or PostgreSQL?

Enter **DuckDB**, an embedded analytical database that is rapidly becoming the "SQLite for Analytics." Below is a guide on why you should use it and how to get started, featuring examples from recent NDC and ArjanCodes tutorials.

---

### What is DuckDB?

DuckDB is a relational, in-process database management system designed for **analytical workloads** (OLAP). Unlike MySQL, which is optimized for small, frequent transactions, DuckDB is optimized for massive aggregations and joins on millions (or even billions) of rows.

**Key Features:**

* **Embedded:** No server to install. It runs inside your application process.
* **Columnar Storage:** Optimized for reading only the columns you need, making it incredibly fast for math-heavy queries.
* **Portable:** The entire database can be stored in a single `.db` file.
* **Hyper-Compatible:** It reads CSV, JSON, Parquet, and even Excel files directly.

---

### 1. Running SQL on Pandas DataFrames

One of DuckDB's "killer features" is its seamless integration with Python. You can actually run standard SQL queries directly on Pandas DataFrames without any extra setup.

```python
import duckdb
import pandas as pd

# Create a simple DataFrame
df = pd.DataFrame({'name': ['Alice', 'Bob'], 'salary': [120000, 90000]})

# Query the DataFrame directly as if it were a table
result = duckdb.query("SELECT * FROM df WHERE salary > 100000").to_df()
print(result)

```

*Reference: [ArjanCodes - Stop Struggling with DataFrames*](https://www.youtube.com/watch?v=8SYQtpSk_OI)

**Pro Tip:** While DuckDB can "magically" find your Python variables, it's best practice to register them explicitly to keep your code clean and your IDE happy:

```python
con = duckdb.connect()
con.register('employees', df)
con.execute("SELECT * FROM employees").fetchdf()

```

---

### 2. High-Performance Analytics on "Medium Data"

In his NDC Oslo talk, David Ostrovsky demonstrated that DuckDB can handle **1 billion rows** on a single laptop in seconds—a feat that would likely crash a standard MySQL instance.

**Example: A Complex Pivot Query**
DuckDB supports advanced SQL syntax like `PIVOT`, which is perfect for generating reports:

```sql
-- This query aggregates and pivots 1 billion rows in ~15 seconds
PIVOT lineitem ON returnflag USING SUM(extendedprice);

```

*Reference: [NDC Oslo - Analytics for not-so-big data*](https://www.youtube.com/watch?v=3iy9qaGopyw)

---

### 3. Querying Files Remotely (S3 & HTTP)

You don't even need to download your data to query it. DuckDB can reach out to a Parquet file on GitHub or an S3 bucket and only pull the specific bytes it needs to answer your query.

```sql
-- Querying a remote Parquet file directly
SELECT count(*) FROM 'https://raw.githubusercontent.com/.../trips.parquet';

```

Because Parquet is a columnar format, DuckDB only reads the "metadata" and the specific column needed, often transferring just a few kilobytes for a multi-megabyte file.

---

### 4. Zero-Setup ETL (Extract, Transform, Load)

Need to turn a 7GB CSV into a compressed Parquet file? DuckDB can do this with a single command from your terminal:

```bash
# Convert CSV to Parquet via the CLI
duckdb -c "COPY (SELECT * FROM read_csv_auto('data.csv')) TO 'data.parquet' (FORMAT PARQUET);"

```

This is often 10x-20x faster than writing a custom script in Python or Pandas because DuckDB is built in C++ and uses every core of your CPU.

---

### 5. Analyzing Your Data

DuckDB includes powerful helper statements to help you understand your data without writing long `SELECT` statements:

* **`DESCRIBE table_name`**: Shows column names and types.
* **`SUMMARIZE table_name`**: Provides a statistical overview (min, max, average, percentiles) of every column.
* **`EXPLAIN ANALYZE query`**: Shows exactly how the database is executing your query and where the bottlenecks are.

### Summary

Whether you are a data scientist tired of complex Pandas syntax or a software engineer needing a fast local analytical engine, DuckDB is a "must-have" in your toolkit. It bridges the gap between simple SQLite and massive distributed clusters, making "medium data" a breeze to handle.

**Watch the full tutorials here:**

* [Analytics for not-so-big data with DuckDB (NDC Oslo)](https://www.youtube.com/watch?v=3iy9qaGopyw)
* [Stop Struggling with DataFrames – Try DuckDB (ArjanCodes)](https://www.youtube.com/watch?v=8SYQtpSk_OI)
